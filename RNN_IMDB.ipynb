{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4lZPFTGYlBUj/X/5ffp/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamaachi/MyDataScience_Projects/blob/main/RNN_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U_qZsIRtVc-B"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# 1. Chargement et Exploration du Dataset IMDB\n",
        "# -------------------------------------------------------------------------------\n",
        "# Le dataset IMDB contient 50 000 critiques de films étiquetées comme positives (1) ou négatives (0).\n",
        "# Il est pré-traité pour inclure uniquement les mots les plus fréquents.\n",
        "\n",
        "# Définir le nombre maximal de mots à considérer. Les mots moins fréquents seront ignorés.\n",
        "# Choisir une valeur raisonnable permet de limiter la taille du vocabulaire et d'améliorer\n",
        "# potentiellement les performances en se concentrant sur les mots les plus importants.\n",
        "num_words = 10000"
      ],
      "metadata": {
        "id": "SxyTvmiGVwl4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le dataset IMDB en utilisant la fonction fournie par Keras.\n",
        "# `load_data` renvoie deux tuples : (x_train, y_train) et (x_test, y_test).\n",
        "# `x_train` et `x_test` sont des listes de séquences, où chaque séquence représente une critique\n",
        "# et est constituée d'indices de mots. `y_train` et `y_test` sont les étiquettes correspondantes.\n",
        "# `num_words=num_words` limite le vocabulaire aux `num_words` mots les plus fréquents.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH9tNO6CV1bu",
        "outputId": "418e1ef6-78a2-4883-bf7f-19cb12a77f02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher quelques informations sur les données chargées.\n",
        "print(\"Nombre de séquences d'entraînement:\", len(x_train))\n",
        "print(\"Nombre de séquences de test:\", len(x_test))\n",
        "print(\"Exemple de séquence d'entraînement (indices de mots):\", x_train[0])\n",
        "print(\"Étiquette correspondante:\", y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjKM_Ah5W1DB",
        "outputId": "f63a0ed8-b830-4560-8b78-67db98cfe148"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de séquences d'entraînement: 25000\n",
            "Nombre de séquences de test: 25000\n",
            "Exemple de séquence d'entraînement (indices de mots): [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "Étiquette correspondante: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour comprendre le contenu des séquences, on peut décoder la première critique\n",
        "# en mots lisibles en utilisant le dictionnaire de mapping des indices de mots.\n",
        "word_index = imdb.get_word_index()\n",
        "# print(word_index)\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[0]])\n",
        "print(\"Exemple de critique d'entraînement décodée:\", decoded_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw5-ubH6W_Ru",
        "outputId": "0b436420-3487-4865-c2d4-f04ac9e9ed8e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemple de critique d'entraînement décodée: ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# 2. Pré-traitement des Données Séquentielles\n",
        "# -------------------------------------------------------------------------------\n",
        "# Les séquences dans le dataset IMDB ont des longueurs variables. Pour que notre modèle RNN\n",
        "# puisse les traiter efficacement, nous devons les rendre de longueur uniforme.\n",
        "# Nous allons utiliser le padding pour cela.\n",
        "\n",
        "# Définir la longueur maximale souhaitée pour toutes les séquences.\n",
        "# Les séquences plus longues seront tronquées, et les séquences plus courtes seront complétées (padding).\n",
        "# Le choix de `maxlen` est crucial et peut influencer les performances du modèle.\n",
        "# Une valeur trop petite peut entraîner une perte d'informations, tandis qu'une valeur trop grande\n",
        "# peut augmenter la complexité du modèle et le temps d'entraînement.\n",
        "maxlen = 200"
      ],
      "metadata": {
        "id": "IFpXfUYA06Qs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utiliser la fonction `pad_sequences` de Keras pour uniformiser la longueur des séquences.\n",
        "# `padding='post'` signifie que le padding sera ajouté à la fin des séquences.\n",
        "# `truncating='post'` signifie que les séquences plus longues seront tronquées à la fin.\n",
        "x_train_padded = sequence.pad_sequences(x_train, maxlen=maxlen, padding='post', truncating='post')\n",
        "x_test_padded = sequence.pad_sequences(x_test, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "print(\"Séquence d'entraînement après padding (exemple):\", x_train_padded[0])\n",
        "print(\"Taille du tenseur des données d'entraînement après padding:\", x_train_padded.shape)\n",
        "print(\"Taille du tenseur des données de test après padding:\", x_test_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCg-Mkkw1LtA",
        "outputId": "c515e726-d5dc-4fad-d68e-59414ecb26f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Séquence d'entraînement après padding (exemple): [   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
            "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
            "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
            "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
            " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
            "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
            "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
            " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
            "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
            "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
            "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
            "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
            " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
            "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
            "    4  226   65   16]\n",
            "Taille du tenseur des données d'entraînement après padding: (25000, 200)\n",
            "Taille du tenseur des données de test après padding: (25000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# 3. Construction du Modèle RNN (LSTM)\n",
        "# -------------------------------------------------------------------------------\n",
        "# Nous allons construire un modèle RNN simple en utilisant une couche LSTM.\n",
        "# LSTM (Long Short-Term Memory) est un type de RNN capable d'apprendre les dépendances à long terme,\n",
        "# ce qui est crucial pour comprendre le sens des phrases dans les critiques de films.\n",
        "\n",
        "# Initialiser un modèle séquentiel. Un modèle séquentiel est une pile linéaire de couches.\n",
        "model = Sequential()"
      ],
      "metadata": {
        "id": "0CTOaSXw1kBD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Couche d'Embedding:\n",
        "# Cette couche transforme les indices de mots en vecteurs denses de taille fixe (embeddings).\n",
        "# `input_dim`: Taille du vocabulaire (nombre total de mots uniques).\n",
        "# `output_dim`: Dimension de l'espace d'embedding (taille des vecteurs de mots).\n",
        "# `input_length`: Longueur des séquences d'entrée (après padding).\n",
        "embedding_dim = 128\n",
        "model.add(Embedding(num_words, embedding_dim, input_length=maxlen))"
      ],
      "metadata": {
        "id": "rXI_Mz--1uP4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Couche LSTM:\n",
        "# Une couche LSTM traite les séquences d'entrée et maintient un état interne (mémoire)\n",
        "# pour capturer les informations séquentielles.\n",
        "# `units`: Nombre d'unités de mémoire dans la couche LSTM (dimension de l'espace caché).\n",
        "lstm_units = 128\n",
        "model.add(LSTM(lstm_units))"
      ],
      "metadata": {
        "id": "6BDZ4gvO2C0e"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Couche Dense:\n",
        "# Une couche dense (fully connected) pour la classification.\n",
        "# `units`: Nombre de neurones dans la couche. Pour une classification binaire, on utilise 1 neurone.\n",
        "# `activation='sigmoid'`: Fonction d'activation sigmoïde qui produit une probabilité entre 0 et 1,\n",
        "# représentant la probabilité que la critique soit positive.\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "6w6lke602Qx9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher un résumé de l'architecture du modèle.\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "KvI6g9q92X0t",
        "outputId": "89b794cb-f275-48f1-9ca8-4eb851e55ff3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# 4. Compilation du Modèle\n",
        "# -------------------------------------------------------------------------------\n",
        "# Avant d'entraîner le modèle, nous devons le compiler en spécifiant l'optimiseur,\n",
        "# la fonction de perte et les métriques à suivre.\n",
        "\n",
        "# Optimiseur:\n",
        "# `adam` est un optimiseur adaptatif couramment utilisé qui ajuste les taux d'apprentissage\n",
        "# des différents paramètres du modèle pendant l'entraînement.\n",
        "optimizer = 'adam'"
      ],
      "metadata": {
        "id": "-yzsas_d2qZN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction de Perte:\n",
        "# `binary_crossentropy` est la fonction de perte appropriée pour les problèmes de classification binaire.\n",
        "loss = 'binary_crossentropy'"
      ],
      "metadata": {
        "id": "9pPFQjLm22As"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Métriques:\n",
        "# `accuracy` est une métrique courante pour évaluer les performances d'un modèle de classification.\n",
        "metrics = ['accuracy']"
      ],
      "metadata": {
        "id": "Qo9e5bqM5QZR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiler le modèle en utilisant les configurations spécifiées.\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "metadata": {
        "id": "eEcJ7HSx5ZO-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# 5. Entraînement du Modèle\n",
        "# -------------------------------------------------------------------------------\n",
        "# Nous allons entraîner le modèle sur les données d'entraînement et évaluer ses performances\n",
        "# sur les données de validation pendant l'entraînement.\n",
        "\n",
        "# Définir le nombre d'époques (passages complets sur l'ensemble d'entraînement).\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "aaFlRwx25c6G"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir la taille du lot (nombre d'échantillons traités à la fois pendant l'entraînement).\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "MfzJrnMU5j22"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utiliser EarlyStopping pour arrêter l'entraînement si la performance sur l'ensemble de validation\n",
        "# cesse de s'améliorer, ce qui aide à prévenir le surapprentissage.\n",
        "# `monitor='val_loss'`: Surveiller la perte sur l'ensemble de validation.\n",
        "# `patience=2`: Arrêter si la perte ne s'améliore pas pendant 2 époques consécutives.\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "X_2YMiL25oQA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diviser les données d'entraînement en un ensemble d'entraînement et un ensemble de validation.\n",
        "# L'ensemble de validation est utilisé pour surveiller les performances du modèle pendant l'entraînement\n",
        "# et pour aider à ajuster les hyperparamètres et prévenir le surapprentissage.\n",
        "validation_split = 0.4"
      ],
      "metadata": {
        "id": "ijYXMPgD51A2"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraîner le modèle en utilisant les données d'entraînement et en validant sur une partie\n",
        "# des données d'entraînement.\n",
        "history = model.fit(x_train_padded, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_split=validation_split,\n",
        "                    callbacks=[early_stopping]\n",
        "                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zG7MOGm55ym",
        "outputId": "55526dd6-960f-4276-9d21-f4c575450634"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9975 - loss: 0.0161 - val_accuracy: 0.9172 - val_loss: 0.4503\n",
            "Epoch 2/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0106 - val_accuracy: 0.9204 - val_loss: 0.4562\n",
            "Epoch 3/20\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9979 - loss: 0.0132 - val_accuracy: 0.9200 - val_loss: 0.4695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# 6. Évaluation du Modèle\n",
        "# -------------------------------------------------------------------------------\n",
        "# Après l'entraînement, nous évaluons les performances du modèle sur l'ensemble de test\n",
        "# pour obtenir une estimation de sa capacité à généraliser à de nouvelles données.\n",
        "\n",
        "# Évaluer le modèle sur les données de test.\n",
        "loss, accuracy = model.evaluate(x_test_padded, y_test, verbose=0)\n",
        "print(\"Perte sur l'ensemble de test:\", loss)\n",
        "print(\"Précision sur l'ensemble de test:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUfqd7Pn6Pmz",
        "outputId": "1fc0cc2e-195a-420a-9386-ac7b28e3ae55"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perte sur l'ensemble de test: 0.9880833029747009\n",
            "Précision sur l'ensemble de test: 0.8170400261878967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# 7. Faire des Prédictions (Optionnel)\n",
        "# -------------------------------------------------------------------------------\n",
        "# Une fois le modèle entraîné et évalué, nous pouvons l'utiliser pour faire des prédictions\n",
        "# sur de nouvelles données.\n",
        "\n",
        "# Faire des prédictions sur l'ensemble de test. Cela renverra des probabilités entre 0 et 1.\n",
        "predictions = model.predict(x_test_padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-xYh8CH6boJ",
        "outputId": "86474620-37bb-4f99-de66-4cad74755efc"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour obtenir des étiquettes binaires (0 ou 1), nous pouvons seuiller les probabilités.\n",
        "# Par exemple, si la probabilité est supérieure à 0.5, on la considère comme une critique positive (1).\n",
        "binary_predictions = (predictions > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "PsdAQ7xK6hDN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher quelques prédictions.\n",
        "print(\"Quelques prédictions (probabilités):\", predictions[:10])\n",
        "print(\"Quelques prédictions binaires:\", binary_predictions[:10].flatten())\n",
        "print(\"Vraies étiquettes pour ces exemples:\", y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAzCmdeR6sIh",
        "outputId": "61fc4c84-7b05-49e1-bced-f50d2b1ca8a1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quelques prédictions (probabilités): [[0.03688854]\n",
            " [0.9916578 ]\n",
            " [0.68579817]\n",
            " [0.2574341 ]\n",
            " [0.9482856 ]\n",
            " [0.80122745]\n",
            " [0.6805467 ]\n",
            " [0.05167335]\n",
            " [0.9411784 ]\n",
            " [0.94827366]]\n",
            "Quelques prédictions binaires: [0 1 1 0 1 1 1 0 1 1]\n",
            "Vraies étiquettes pour ces exemples: [0 1 1 0 1 1 1 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# 7.A. Fonction pour Faire une Prédiction sur une Nouvelle Phrase\n",
        "# -------------------------------------------------------------------------------\n",
        "\"\"\"\n",
        "    Prédit le sentiment d'une phrase donnée en utilisant le modèle entraîné.\n",
        "\n",
        "    Args:\n",
        "        text (str): La phrase à analyser.\n",
        "\n",
        "    Returns:\n",
        "        str: \"Positive\" ou \"Negative\" indiquant le sentiment prédit.\n",
        "\"\"\"\n",
        "def predict_sentiment(text):\n",
        "\n",
        "    # 1. Prétraitement de la nouvelle phrase:\n",
        "    #    - Convertir la phrase en minuscules (bonne pratique pour l'uniformité).\n",
        "    #    - Tokenisation: Séparer la phrase en mots.\n",
        "    #      Ici, on utilise une simple division par les espaces, mais pour un traitement\n",
        "    #      plus robuste, on pourrait utiliser un tokenizer plus avancé (comme celui de nltk ou spaCy).\n",
        "    words = text.lower().split()\n",
        "\n",
        "    #    - Conversion des mots en indices en utilisant le vocabulaire du dataset IMDB.\n",
        "    #      Les mots qui ne sont pas dans le vocabulaire seront ignorés (ou mappés à un indice inconnu si géré).\n",
        "    word_indices = [word_index.get(word, 0) + 3 for word in words]  # +3 car les indices 0, 1, 2 sont réservés\n",
        "\n",
        "    #    - Padding ou troncature de la séquence pour correspondre à la longueur attendue par le modèle.\n",
        "    padded_sequence = sequence.pad_sequences([word_indices], maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "    # 2. Faire la prédiction avec le modèle:\n",
        "    prediction = model.predict(padded_sequence)\n",
        "\n",
        "    # 3. Interpréter la prédiction:\n",
        "    #    - La sortie du modèle est une probabilité entre 0 et 1.\n",
        "    #    - Un seuil (généralement 0.5) est utilisé pour classer le sentiment.\n",
        "    if prediction[0] > 0.5:\n",
        "        return \"Positive\"\n",
        "    else:\n",
        "        return \"Negative\""
      ],
      "metadata": {
        "id": "nIgRqkC78Di6"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# 7.B. Tester la Fonction de Prédiction\n",
        "# -------------------------------------------------------------------------------\n",
        "\n",
        "# Exemples de phrases à tester\n",
        "positive_review = \"This movie was absolutely fantastic! The acting, the plot, everything was perfect.\"\n",
        "negative_review = \"I was really disappointed by this film. The story was boring and the actors were unconvincing.\"\n",
        "neutral_review = \"The movie was okay, nothing special but not terrible either.\" # Exemple d'une phrase plus neutre\n",
        "\n",
        "# Faire des prédictions et afficher les résultats\n",
        "print(f\"Sentiment de la phrase positive: {predict_sentiment(positive_review)}\")\n",
        "print(f\"Sentiment de la phrase négative: {predict_sentiment(negative_review)}\")\n",
        "print(f\"Sentiment de la phrase neutre: {predict_sentiment(neutral_review)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT6__Jit8r-s",
        "outputId": "69b16143-ea8c-4614-927c-d9a5bc1e7baf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Sentiment de la phrase positive: Positive\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Sentiment de la phrase négative: Negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Sentiment de la phrase neutre: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 8. Bonnes Pratiques comme un Data Scientist Senior\n",
        "\n",
        "\n",
        "# 8.1. Exploration Approfondie des Données (EDA):\n",
        "Avant même de commencer le prétraitement, un data scientist senior passerait\n",
        "du temps à explorer le dataset. Cela inclut :\n",
        "- Visualiser la distribution des longueurs des séquences pour prendre une\n",
        "décision éclairée sur `maxlen`.\n",
        "- Analyser la fréquence des mots pour comprendre le vocabulaire et potentiellement\n",
        "identifier les mots à exclure ou à traiter spécialement.\n",
        "- Examiner des exemples de critiques positives et négatives pour avoir une intuition\n",
        "sur les caractéristiques linguistiques associées à chaque sentiment.\n",
        "\n",
        "# 8.2. Expérimentation avec Différentes Architectures RNN:\n",
        "Un SimpleRNN, un GRU (Gated Recurrent Unit) ou plusieurs couches LSTM pourraient\n",
        "être essayés. Chaque architecture a ses propres forces et faiblesses. Un data\n",
        "scientist senior testerait différentes configurations pour trouver celle qui\n",
        "fonctionne le mieux pour ce problème spécifique.\n",
        "\n",
        "# 8.3. Optimisation des Hyperparamètres:\n",
        "Les hyperparamètres comme `embedding_dim`, `lstm_units`, `batch_size`, le taux\n",
        "d'apprentissage de l'optimiseur, etc., ont un impact significatif sur les\n",
        "performances du modèle. Un data scientist senior utiliserait des techniques\n",
        "comme la validation croisée ou la recherche sur grille (Grid Search) ou la\n",
        "recherche aléatoire (Random Search) pour trouver les valeurs optimales.\n",
        "\n",
        "# 8.4. Utilisation d'un Ensemble de Validation Approprié:\n",
        "S'assurer que l'ensemble de validation est représentatif des données d'entraînement\n",
        "et qu'il n'y a pas de fuite de données entre les ensembles. L'utilisation de\n",
        "`validation_split` est une méthode simple, mais pour des projets plus complexes,\n",
        "une division stratifiée ou une validation croisée pourraient être nécessaires.\n",
        "\n",
        "# 8.5. Techniques de Régularisation:\n",
        "Pour lutter contre le surapprentissage, un data scientist senior utiliserait\n",
        "des techniques de régularisation telles que :\n",
        "- Dropout: Ignorer aléatoirement certains neurones pendant l'entraînement.\n",
        "- L1 ou L2 Regularization: Ajouter une pénalité à la fonction de perte basée\n",
        "sur la magnitude des poids du modèle.\n",
        "\n",
        "# 8.6. Gestion du Vocabulaire et des Mots Hors Vocabulaire (OOV):\n",
        "Décider comment traiter les mots qui n'apparaissent pas dans le vocabulaire\n",
        "limité par `num_words`. Des techniques comme l'utilisation d'un jeton spécial\n",
        "pour les mots OOV ou l'augmentation du vocabulaire pourraient être envisagées.\n",
        "\n",
        "# 8.7. Analyse des Erreurs:\n",
        "Après l'entraînement, examiner les exemples où le modèle s'est trompé. Cela\n",
        "peut fournir des indices sur les aspects que le modèle a du mal à apprendre\n",
        "et guider les améliorations futures.\n",
        "\n",
        "# 8.8. Pipelines de Données et Reproductibilité:\n",
        "Utiliser des pipelines de données pour organiser le prétraitement et s'assurer\n",
        "de la reproductibilité des expériences. Cela implique de structurer le code\n",
        "de manière modulaire et d'utiliser des outils de gestion des expériences.\n",
        "\n",
        "# 8.9. Documentation et Commentaires:\n",
        "Écrire un code clair, bien commenté et documenté est essentiel pour la collaboration\n",
        "et la maintenance du projet.\n",
        "\n",
        "# 8.10. Versionnement du Code et des Modèles:\n",
        "Utiliser un système de contrôle de version (comme Git) pour suivre les modifications\n",
        "du code et des modèles. Cela permet de revenir à des versions précédentes si nécessaire\n",
        "et de collaborer efficacement.\n",
        "\n",
        "En appliquant ces pratiques, un data scientist senior peut construire des modèles\n",
        "plus robustes, plus performants et plus faciles à maintenir. Le code ci-dessus\n",
        "fournit une base solide, mais l'exploration, l'expérimentation et l'itération\n",
        "sont des étapes clés pour atteindre des résultats optimaux."
      ],
      "metadata": {
        "id": "Fr1GpYRu7Wvq"
      }
    }
  ]
}